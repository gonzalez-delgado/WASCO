{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4603374d",
   "metadata": {},
   "source": [
    "# Compute Wasserstein matrix between two ensembles\n",
    "## Difference between the empirical global structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be399c2d",
   "metadata": {},
   "source": [
    "Important note: Python version needs to be set to __Python 3.8__ to be able to load the library `faiss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76368bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py\n",
    "import warnings # Optional\n",
    "warnings.filterwarnings(\"ignore\") # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf9555",
   "metadata": {},
   "source": [
    "The function `wasserstein_ij` computes __Wasserstein distance__ between the relative i,j position distributions (the i,j element of the __empirical global structures__) of a pair of (replicas of two) ensembles. This function will be parallelized across the list of all pairwise positions along the sequence. Its arguments are:\n",
    "\n",
    "* `which_pos`: a tuple of two integers `tuple((pos_i, pos_j))`, where `pos_i` and `pos_j` denote the i-th and j-th sequence positions respectively.\n",
    "\n",
    "* `prot_name_1`: the name of the first ensemble, whose .hdf5 coordinates file is `prot_name_1_coordinates.hdf5`.\n",
    "\n",
    "* `prot_name_2`: the name of the second ensemble, whose .hdf5 coordinates file is `prot_name_2_coordinates.hdf5`.\n",
    "\n",
    "* `ncenters`: the number of clusters when kmeans clustering needs to be performed.\n",
    "\n",
    "* `coor_path`: the path where all the coordinates .hdf5 files are located.\n",
    "\n",
    "Normally, the user can load `wasserstein_ij` and skip to the next function `w_matrix`, which integrates `wasserstein_ij` and its arguments, and should be used if the complete matrix for all pairwise relative positions need to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515dccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_ij(which_pos, prot_name_1, prot_name_2, ncenters, coor_path):\n",
    "    \n",
    "    # Required libraries (need to be placed here for parallel computing)\n",
    "   \n",
    "    import faiss # Needs Python 3.8\n",
    "    import h5py\n",
    "    import os\n",
    "    import ot\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "           \n",
    "    ########################################\n",
    "    \n",
    "    # Extract relative i,j positions distributions for both ensembles\n",
    "    \n",
    "    os.chdir(coor_path)\n",
    "  \n",
    "    h5f_1 = h5py.File(\"_\".join([prot_name_1,'coordinates.hdf5']),'r')\n",
    "    L1 = int(0.5*(1 + np.sqrt(1 + 8*np.shape(h5f_1['ensemble'])[1]))) # Sequence length\n",
    "    pos_pairs_1 = list(itertools.combinations(range(L1), 2)) # List of pairwise positions\n",
    "    pos_ij_prot_1 = h5f_1['ensemble'][:, pos_pairs_1.index(which_pos), :] # Access to i,j relative position distribution\n",
    "    h5f_1.close()\n",
    "\n",
    "    h5f_2 = h5py.File(\"_\".join([prot_name_2,'coordinates.hdf5']),'r')\n",
    "    L2 = int(0.5*(1 + np.sqrt(1 + 8*np.shape(h5f_2['ensemble'])[1]))) # Sequence length\n",
    "    pos_pairs_2 = list(itertools.combinations(range(L2), 2)) # List of pairwise positions\n",
    "    pos_ij_prot_2 = h5f_2['ensemble'][:, pos_pairs_2.index(which_pos), :] # Access to i,j relative position distribution\n",
    "    h5f_2.close()\n",
    "    \n",
    "    pos_ij_prot_1 = pos_ij_prot_1[~np.isnan(pos_ij_prot_1).any(axis=1),:] # Remove missing conformations\n",
    "    pos_ij_prot_2 = pos_ij_prot_2[~np.isnan(pos_ij_prot_2).any(axis=1),:] \n",
    "       \n",
    "    if L1 != L2: \n",
    "        quit('Both ensembles must have the same length') \n",
    "    \n",
    "    n = np.shape(pos_ij_prot_1)[0] # Number of conformations in the first ensemble\n",
    "    m = np.shape(pos_ij_prot_2)[0] # Number of conformations in the second ensemble\n",
    "    \n",
    "    # Keeping (x,y,z) coordinates for clustering in R3\n",
    "    \n",
    "    pos_ij_prot_1 = pos_ij_prot_1[:,0:3]\n",
    "    pos_ij_prot_2 = pos_ij_prot_2[:,0:3]\n",
    "        \n",
    "    # Check is clustering is needed\n",
    "    \n",
    "    if n <= ncenters: # No clustering for the first ensemble\n",
    "        \n",
    "        a = pos_ij_prot_1\n",
    "        ma = np.ones(n)/n\n",
    "            \n",
    "    else: # Clustering for the first ensemble\n",
    "         \n",
    "        kmeans_1 = faiss.Kmeans(d = 3, k = ncenters, min_points_per_centroid = 1, max_points_per_centroid = 10000000)\n",
    "        kmeans_1.train(pos_ij_prot_1.astype(np.float32))\n",
    "            \n",
    "        #Check for empty clusters\n",
    "        kmeans_labels = kmeans_1.index.search(pos_ij_prot_1.astype(np.float32),1)[1]\n",
    "        empty_clusters = np.setdiff1d(np.arange(ncenters), kmeans_labels)\n",
    "       \n",
    "        a = kmeans_1.centroids # Support of the \"clustered distribution\"\n",
    "        mass_a = pd.DataFrame(kmeans_labels).value_counts().sort_index() \n",
    "        ma = np.array(mass_a/np.sum(mass_a)) # Probability mass assigned to each cluster\n",
    "            \n",
    "        if len(empty_clusters) > 0:\n",
    "            \n",
    "            a = np.delete(a, empty_clusters, axis = 0) # Remove empty clusters\n",
    "      \n",
    "    if m <= ncenters: # No clustering for the second ensemble\n",
    "        \n",
    "        b = pos_ij_prot_2\n",
    "        mb = np.ones(m)/m\n",
    "    \n",
    "    else: # Clustering for the second ensemble\n",
    "               \n",
    "        kmeans_2 = faiss.Kmeans(d = 3, k = ncenters, min_points_per_centroid = 1, max_points_per_centroid = 10000000)\n",
    "        kmeans_2.train(pos_ij_prot_2.astype(np.float32))\n",
    "            \n",
    "        #Check for empty clusters\n",
    "        kmeans_labels = kmeans_2.index.search(pos_ij_prot_2.astype(np.float32),1)[1]\n",
    "        empty_clusters = np.setdiff1d(np.arange(ncenters), kmeans_labels)\n",
    "       \n",
    "        b = kmeans_2.centroids # Support of the \"clustered distribution\"\n",
    "        mass_b = pd.DataFrame(kmeans_labels).value_counts().sort_index()\n",
    "        mb = np.array(mass_b/np.sum(mass_b)) # Probability mass assigned to each cluster\n",
    "            \n",
    "        if len(empty_clusters) > 0:\n",
    "            \n",
    "            b = np.delete(b, empty_clusters, axis = 0) # Remove empty clusters\n",
    "      \n",
    "         \n",
    "    # Compute Wasserstein distance     \n",
    "         \n",
    "    M = ot.dist(a, b, metric = 'sqeuclidean') # Cost matrix\n",
    "    clean = ot.utils.clean_zeros(ma, mb, M)\n",
    "    w_ij = np.sqrt(ot.emd2(clean[0], clean[1], clean[2])) # 2-Wasserstein distance\n",
    "        \n",
    "    return w_ij\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f524e",
   "metadata": {},
   "source": [
    "The function `w_matrix` parallelizes `wasserstein_ij` across the list of all pairwise relative positions along the sequence. Therefore, it computes the Wasserstein matrix representing the difference between the empirical global structures of a pair of (replicas of two) ensembles. The function returns an array of shape [number of position pairs, 3] array, ready to be graphically represented (using function `plot_matrix`). For a given pair of positions i,j, the array's element [s,:], where s is given by\n",
    "\n",
    "    L = 10 # Sequence length\n",
    "    pos_pairs = list(itertools.combinations(range(L), 2)) # Pairwise positions \n",
    "    s = pos_pairs.index(tuple((i,j)))\n",
    "    \n",
    "contains the vector `[pos_i, pos_j, w_ij]`, where `pos_i` and `pos_j` are respectively the i-th and j-th sequence positions (starting form zero), and `w_ij` the computed Wasserstein distance between the i,j relative position distributions of both ensembles.\n",
    "\n",
    "\n",
    "The arguments of `w_matrix` are:\n",
    "\n",
    "* `prot_name_1`: the name of the first ensemble, whose .hdf5 coordinates file is `prot_name_1_coordinates.hdf5`.\n",
    "\n",
    "* `prot_name_2`: the name of the second ensemble, whose .hdf5 coordinates file is `prot_name_2_coordinates.hdf5`.\n",
    "\n",
    "* `N_centers`: the number of clusters when kmeans clustering needs to be performed.\n",
    "\n",
    "* `data_path`: the path where all the coordinates .hdf5 files are located.\n",
    "\n",
    "The computation time may be long, depending on both ensemble sizes (sequence length, number of conformations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aeaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_matrix(prot_1, prot_2, N_centers, N_cores, data_path):\n",
    "    \n",
    "    \n",
    "    # Some data need to be loaded to get basic parameters\n",
    "    os.chdir(data_path) \n",
    "    h5f_1 = h5py.File(\"_\".join([prot_1,'coordinates.hdf5']),'r')\n",
    "    L1 = int(0.5*(1 + np.sqrt(1 + 8*np.shape(h5f_1['ensemble'])[1]))) # Sequence length\n",
    "    it_pairs = list(itertools.combinations(range(L1), 2)) # List of all pairwise relative positions\n",
    "    del(h5f_1) # Clear memory\n",
    "    \n",
    "    it_function = partial(wasserstein_ij, prot_name_1 = prot_1, prot_name_2 = prot_2,\n",
    "                         ncenters = N_centers, coor_path = data_path) \n",
    "      \n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "    print(\"\".join(['Computing pairwise Wasserstein distances for ', str(len(it_pairs)), ' pairs of sequence positions.\\n']))\n",
    "    print(\"\".join(['Protein 1 : ', prot_1,'\\n']))\n",
    "    print(\"\".join(['Protein 2 : ', prot_2,'\\n']))\n",
    "    print('-------------------------------------------------------------------\\n')   \n",
    "    \n",
    "    if __name__ == \"__main__\": # Parallel computing\n",
    "        pairwise_distances = Parallel(n_jobs = N_cores, verbose=10, backend = 'threading')(delayed(it_function)(i) for i in it_pairs)\n",
    "    \n",
    "    pairs = np.asarray(it_pairs)\n",
    "    distances = np.reshape(np.asarray(pairwise_distances), [len(pairwise_distances), 1])\n",
    "    \n",
    "    return np.concatenate([pairs, distances], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a3dff",
   "metadata": {},
   "source": [
    "## Executing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_name_1 = \"my_ensemble_1\"\n",
    "prot_name_2 = \"my_ensemble_2\"\n",
    "coordinates_path = \"/path_to_coordinates_folder\" # Folder where my_ensemble_1_coordinates.hdf5 and my_ensemble_2_coordinates.hdf5 are located\n",
    "\n",
    "n_clusters = 2000 # Recommended number of clusters\n",
    "n_cores = 1 # Number of cores for parallel computing\n",
    "\n",
    "wmatrix = w_matrix(prot_1 = prot_name_1, prot_2 = prot_name_2 , N_centers = n_clusters, N_cores = n_cores, data_path = coordinates_path)\n",
    "\n",
    "# The resulting matrix should be saved (needed for graphic representation)\n",
    "os.chdir('save_in_this_path')\n",
    "np.save(\"_\".join([prot_name_1,prot_name_2,'wmatrix.npy']), wmatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
